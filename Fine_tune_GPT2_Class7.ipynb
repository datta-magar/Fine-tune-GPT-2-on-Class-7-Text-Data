{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8a44ca",
   "metadata": {},
   "source": [
    "# Fine-tune GPT-2 on NCERT Class 7 Text Data\n",
    "This notebook trains a GPT-2 model on Class 7 NCERT content to serve as a personalized teacher/coach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55052894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.6.0.dev20241121)\n",
      "Requirement already satisfied: accelerate in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: matplotlib in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the lines below to install required libraries if not already installed\n",
    "!pip install transformers datasets torch accelerate matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04bc2494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a93f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:19:09.633266 - Loading text data...\n",
      "2024-11-22 18:19:09.656984 - Loaded 71 text files.\n",
      "2024-11-22 18:19:09.668658 - Dataset created with 71 examples.\n"
     ]
    }
   ],
   "source": [
    "# Set paths for data\n",
    "base_dir = \"./pdf_data_to_raw_text\"  # Replace with the directory containing your NCERT text files\n",
    "save_model_dir = \"./class7_gpt2_model\"\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_text_files(base_dir):\n",
    "    \"\"\"Load all text files from a directory and subdirectories.\"\"\"\n",
    "    text_data = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text_data.append(f.read())\n",
    "    return text_data\n",
    "\n",
    "print(f\"{datetime.now()} - Loading text data...\")\n",
    "text_data = load_text_files(base_dir)\n",
    "print(f\"{datetime.now()} - Loaded {len(text_data)} text files.\")\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"text\": text_data})\n",
    "print(f\"{datetime.now()} - Dataset created with {len(dataset)} examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1c8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:19:12.897439 - Loading tokenizer...\n",
      "2024-11-22 18:19:13.842618 - Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 71/71 [00:01<00:00, 60.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:19:15.708638 - Tokenization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(f\"{datetime.now()} - Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token for training\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "print(f\"{datetime.now()} - Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(f\"{datetime.now()} - Tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fd218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long sequences into chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 71/71 [00:00<00:00, 1507.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into chunks done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Chunks: This step ensures all examples in your dataset conform to the model's context window limit.\n",
    "MAX_LENGTH = 1024  # Define the model's maximum sequence length\n",
    "\n",
    "def split_into_chunks(batch):\n",
    "    input_ids = batch['input_ids']\n",
    "    chunks = [input_ids[i:i+MAX_LENGTH] for i in range(0, len(input_ids), MAX_LENGTH)]\n",
    "    return {'input_ids': chunks}\n",
    "\n",
    "print(\"Splitting long sequences into chunks...\")\n",
    "tokenized_dataset = tokenized_dataset.map(split_into_chunks, batched=False)\n",
    "print(\"Splitting into chunks done\")\n",
    "\n",
    "# Formatting Data for Training:\n",
    "# Before training, you'll need to ensure the dataset is flattened, as splitting can create nested lists. Use Dataset.flatten() if necessary:\n",
    "# tokenized_dataset = tokenized_dataset.flatten()\n",
    "# print(\"data flattened successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106a4c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5565 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # GPT-2 is not a masked language model\n",
    ")\n",
    "\n",
    "# Visualize tokenized data distribution\n",
    "token_lengths = [len(tokenizer(text)[\"input_ids\"]) for text in text_data]\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de46048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:19:26.565407 - Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:19:29.633692 - Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 Model\n",
    "print(f\"{datetime.now()} - Loading GPT-2 model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust token embeddings\n",
    "print(f\"{datetime.now()} - Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a30224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:20:34.837775 - Training arguments configured.\n"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=save_model_dir,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "#     save_steps=10_000,\n",
    "#     save_total_limit=2,\n",
    "#     fp16=True,  # Mixed precision training for faster computation on M1\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_model_dir,\n",
    "    eval_strategy=\"epoch\",  # Keep evaluation if required\n",
    "    save_strategy=\"epoch\",        # Align save strategy with evaluation\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=False,                   # Disable fp16 for MPS\n",
    "    use_cpu=True                  # Ensures usage of MPS backend instead of CUDA\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{datetime.now()} - Training arguments configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ad3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 71/71 [00:00<00:00, 1607.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Fix: Add Labels to the Dataset\n",
    "# To fix this, you need to add a labels field to your dataset. For language modeling tasks (like fine-tuning GPT-2), the labels are usually the same as the input_ids.\n",
    "\n",
    "# Here’s how to modify the dataset:\n",
    "\n",
    "# Update the Dataset to Include Labels\n",
    "\n",
    "def add_labels(batch):\n",
    "    # Use `input_ids` as labels for causal language modeling\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "# Make sure you apply this update before splitting the dataset or passing it to the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba76ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 56 examples\n",
      "Eval dataset: 15 examples\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset using Hugging Face's train_test_split\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Extract train and evaluation datasets\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Eval dataset: {len(eval_dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f6a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.6.0.dev20241121)\n",
      "Requirement already satisfied: torchvision in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.20.0.dev20241121)\n",
      "Requirement already satisfied: torchaudio in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.5.0.dev20241121)\n",
      "Requirement already satisfied: filelock in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/datta/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "168ad33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device: mps\n"
     ]
    }
   ],
   "source": [
    "# Ensure the Correct Device\n",
    "# Verify that your training script is correctly set up to use \"mps\" as the training device. To explicitly set the device:\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device:\", device)\n",
    "else:\n",
    "    print(\"MPS device not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a93df908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:39:09.515107 - Setting up Trainer...\n",
      "2024-11-22 18:39:09.536308 - Trainer setup complete.\n",
      "2024-11-22 18:39:09.536373 - Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 93%|█████████▎| 39/42 [22:34<01:19, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8154, 'grad_norm': 6.206544876098633, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 93%|█████████▎| 39/42 [24:29<01:19, 26.62s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.950634002685547, 'eval_runtime': 8.8047, 'eval_samples_per_second': 1.704, 'eval_steps_per_second': 0.454, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 93%|█████████▎| 39/42 [27:09<01:19, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7329, 'grad_norm': 3.80417537689209, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 93%|█████████▎| 39/42 [30:46<01:19, 26.62s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.911417245864868, 'eval_runtime': 9.1438, 'eval_samples_per_second': 1.64, 'eval_steps_per_second': 0.437, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 93%|█████████▎| 39/42 [31:41<01:19, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7229, 'grad_norm': 3.7061266899108887, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 93%|█████████▎| 39/42 [36:02<01:19, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6599, 'grad_norm': 3.821566581726074, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 93%|█████████▎| 39/42 [37:06<01:19, 26.62s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.89694881439209, 'eval_runtime': 11.019, 'eval_samples_per_second': 1.361, 'eval_steps_per_second': 0.363, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "                                               \n",
      "100%|██████████| 42/42 [19:02<00:00, 27.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1142.4692, 'train_samples_per_second': 0.147, 'train_steps_per_second': 0.037, 'train_loss': 2.7262948808215914, 'epoch': 3.0}\n",
      "2024-11-22 18:58:12.256061 - Training complete.\n",
      "2024-11-22 18:58:12.256263 - Saving model to ./class7_gpt2_model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./class7_gpt2_model/tokenizer_config.json',\n",
       " './class7_gpt2_model/special_tokens_map.json',\n",
       " './class7_gpt2_model/vocab.json',\n",
       " './class7_gpt2_model/merges.txt',\n",
       " './class7_gpt2_model/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Trainer\n",
    "# print(f\"{datetime.now()} - Setting up Trainer...\")\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "# Create Trainer\n",
    "print(f\"{datetime.now()} - Setting up Trainer...\")\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{datetime.now()} - Trainer setup complete.\")\n",
    "\n",
    "\n",
    "# Train Model\n",
    "print(f\"{datetime.now()} - Starting training...\")\n",
    "trainer.train()\n",
    "print(f\"{datetime.now()} - Training complete.\")\n",
    "\n",
    "# Save the model\n",
    "print(f\"{datetime.now()} - Saving model to {save_model_dir}...\")\n",
    "model.save_pretrained(save_model_dir)\n",
    "tokenizer.save_pretrained(save_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "457fec18",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Example Test\u001b[39;00m\n\u001b[1;32m     17\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the properties of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, max_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate response using the trained model.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1030\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# Test the Trained Model\n",
    "def generate_response(prompt, max_length=50):\n",
    "    \"\"\"Generate response using the trained model.\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch.device(\"mps\"))\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Test\n",
    "prompt = \"Explain the properties of integers.\"\n",
    "response = generate_response(prompt)\n",
    "print(\"Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
