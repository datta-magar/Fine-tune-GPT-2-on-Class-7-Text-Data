{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8a44ca",
   "metadata": {},
   "source": [
    "# Fine-tune GPT-2 on NCERT Class 7 Text Data\n",
    "This notebook trains a GPT-2 model on Class 7 NCERT content to serve as a personalized teacher/coach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55052894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below to install required libraries if not already installed\n",
    "# !pip install transformers datasets torch accelerate matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for data\n",
    "base_dir = \"path/to/your/data\"  # Replace with the directory containing your NCERT text files\n",
    "save_model_dir = \"./class7_gpt2_model\"\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_text_files(base_dir):\n",
    "    \"\"\"Load all text files from a directory and subdirectories.\"\"\"\n",
    "    text_data = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text_data.append(f.read())\n",
    "    return text_data\n",
    "\n",
    "print(f\"{datetime.now()} - Loading text data...\")\n",
    "text_data = load_text_files(base_dir)\n",
    "print(f\"{datetime.now()} - Loaded {len(text_data)} text files.\")\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"text\": text_data})\n",
    "print(f\"{datetime.now()} - Dataset created with {len(dataset)} examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"{datetime.now()} - Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token for training\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "print(f\"{datetime.now()} - Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(f\"{datetime.now()} - Tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # GPT-2 is not a masked language model\n",
    ")\n",
    "\n",
    "# Visualize tokenized data distribution\n",
    "token_lengths = [len(tokenizer(text)[\"input_ids\"]) for text in text_data]\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de46048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Model\n",
    "print(f\"{datetime.now()} - Loading GPT-2 model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust token embeddings\n",
    "print(f\"{datetime.now()} - Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a30224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_model_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Mixed precision training for faster computation on M1\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"{datetime.now()} - Training arguments configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93df908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "print(f\"{datetime.now()} - Setting up Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "print(f\"{datetime.now()} - Starting training...\")\n",
    "trainer.train()\n",
    "print(f\"{datetime.now()} - Training complete.\")\n",
    "\n",
    "# Save the model\n",
    "print(f\"{datetime.now()} - Saving model to {save_model_dir}...\")\n",
    "model.save_pretrained(save_model_dir)\n",
    "tokenizer.save_pretrained(save_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Trained Model\n",
    "def generate_response(prompt, max_length=50):\n",
    "    \"\"\"Generate response using the trained model.\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch.device(\"mps\"))\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Test\n",
    "prompt = \"Explain the properties of integers.\"\n",
    "response = generate_response(prompt)\n",
    "print(\"Response:\", response)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
